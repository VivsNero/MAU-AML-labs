{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language translation with Torchtext!! ## \n",
    "\n",
    "Seq2Seq network with torchtext\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/torchtext_translation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys#\n",
    "stdout = sys.stdout#\n",
    "\n",
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import torchtext\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "import io\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import math\n",
    "import time\n",
    "\n",
    "sys.stdout = stdout #these lines are a workaround to fix an issue where no prints were showing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
    "\n",
    "de_tokenizer = get_tokenizer('spacy', language='de')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en')\n",
    "\n",
    "def build_vocab(filepath, tokenizer):\n",
    "    counter = Counter()\n",
    "    with io.open(filepath, encoding=\"utf8\") as f:\n",
    "        for string_ in f:\n",
    "            counter.update(tokenizer(string_))\n",
    "    \n",
    "    return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
    "\n",
    "def data_process(filepaths):\n",
    "    raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "    src_max_length = 0\n",
    "    tgt_max_length = 0\n",
    "    data = []\n",
    "    for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "        de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)], dtype=torch.long)\n",
    "        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long)\n",
    "        data.append((de_tensor_, en_tensor_))\n",
    "        \n",
    "        if de_tensor_.size(0) > src_max_length:\n",
    "            src_max_length = de_tensor_.size(0)\n",
    "            \n",
    "        if en_tensor_.size(0) > tgt_max_length:\n",
    "            tgt_max_length = en_tensor_.size(0)\n",
    "        \n",
    "    return data, src_max_length, tgt_max_length\n",
    "\n",
    "train_data, trsl, trtl = data_process(train_filepaths)\n",
    "val_data, vsl, vtl = data_process(val_filepaths)\n",
    "test_data, tesl, tetl = data_process(test_filepaths)\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "47\n",
      "f\n"
     ]
    }
   ],
   "source": [
    "# We calculate the sentence with max_length\n",
    "candidates_lengths = [trsl, trtl, vsl, vtl, tesl, tetl]\n",
    "print(max(candidates_lengths))\n",
    "MAX_LENGTH = max(candidates_lengths) + 2\n",
    "print(MAX_LENGTH)\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "BOS_IDX = de_vocab['<bos>']\n",
    "EOS_IDX = de_vocab['<eos>']\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    de_batch, en_batch, en_batch_out = [], [], []\n",
    "    a = 0\n",
    "    for (de_item, en_item) in data_batch:\n",
    "        de_extra = MAX_LENGTH - (de_item.size(0) + 2)\n",
    "        en_extra = MAX_LENGTH - (en_item.size(0) + 2)\n",
    "        en_inp_extra = MAX_LENGTH - (en_item.size(0) + 1)\n",
    "        \n",
    "        de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX]), torch.full((de_extra,), PAD_IDX)], dim=0))\n",
    "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX]), torch.full((en_extra,), PAD_IDX)], dim=0))\n",
    "        en_batch_out.append(torch.cat([en_item, torch.tensor([EOS_IDX]), torch.full((en_inp_extra,), PAD_IDX)], dim=0)) # Target input \n",
    "    de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    en_batch_out = pad_sequence(en_batch_out, padding_value=PAD_IDX)\n",
    "    return de_batch, en_batch, en_batch_out\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_id_to_text = lambda x: [de_vocab.itos[t] for t in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_id_to_text = lambda x: [en_vocab.itos[t] for t in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we create the Seq2Seq model ##\n",
    "\n",
    "The Seq2Seq model is an encoder-decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest seq2seq decoder we use only last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder’s last hidden state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, gru_layers):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.gru_layers = gru_layers\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, gru_layers)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size()[0]\n",
    "#         print(\"X :\", x.shape)\n",
    "        embedded = self.embedding(x)\n",
    "#         print(\"EMBEDD: \", embedded.shape)\n",
    "        embedded = embedded.view(1, batch_size, -1)\n",
    "#         print(\"EMBEDD VIEW: \", embedded.shape)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(self.gru_layers, batch_size, self.hidden_size, device=device)\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Decoder ###\n",
    "If only the context vector is passed betweeen the encoder and decoder, that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to “focus” on a different part of the encoder’s outputs for every step of the decoder’s own outputs. First we calculate a set of attention weights. These will be multiplied by the encoder output vectors to create a weighted combination. The result (called attn_applied in the code) should contain information about that specific part of the input sequence, and thus help the decoder choose the right output words.\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward layer attn, using the decoder’s input and hidden state as inputs. Because there are sentences of all sizes in the training data, to actually create and train this layer we have to choose a maximum sentence length (input length, for encoder outputs) that it can apply to. Sentences of the maximum length will use all the attention weights, while shorter sentences will only use the first few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, gru_layers, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.gru_layers = gru_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, gru_layers)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        batch_size = x.size()[0]\n",
    "        # Embedding: a simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "        # This module is often used to store word embeddings and retrieve them using indices. \n",
    "        # The input to the module is a list of indices, and the output is the corresponding word embeddings\n",
    "#         print(\"INPUT SHAPE: \", x.shape)\n",
    "#         print(batch_size)\n",
    "        embedded = self.embedding(x)\n",
    "#         print(\"EMBEDD: \", embedded.shape)\n",
    "        embedded = embedded.view(1, batch_size, -1)\n",
    "#         print(\"EMBEDD VIEW: \", embedded.shape)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        \n",
    "        # torch.bmm performs a batch matrix-matrix product of matrices stored in input and mat2.\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(self.gru_layers, batch_size, self.hidden_size, device=device)\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We  create the classic train/evaluate loop ###\n",
    "\n",
    "For this task, we have something called **teacher_forcing_ratio**, this helps us vary between giving the network the possibility to try to translate using its own previous prediction (no teacher forcing), or we use the known target for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, target_tensor_out, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, initHIDDEN = True):\n",
    "    \n",
    "    if initHIDDEN:\n",
    "        encoder_hidden = encoder.initHidden(input_tensor.size(1))\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    # We run the input sequence through the encoder\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "#         print(encoder_output.shape)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    \n",
    "    decoder_input = torch.full((target_tensor.size(1),), BOS_IDX, device=device)\n",
    "#     decoder_input = torch.tensor([[BOS_IDX], target_tensor.size(1)], device=device)\n",
    "\n",
    "#     print(decoder_input.shape)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoded_words = []\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoded_words.append(english_id_to_text(topi[0]))\n",
    "            loss += criterion(decoder_output, target_tensor_out[di])\n",
    "            if di + 1 < target_length:\n",
    "                decoder_input = target_tensor[di + 1]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoded_words.append(english_id_to_text(topi[0]))\n",
    "            decoder_input = topi.detach()  # detach from history as input\n",
    "#             print(target_tensor[di].shape)\n",
    "            loss += criterion(decoder_output, target_tensor_out[di])\n",
    "\n",
    "#     print(decoded_words)\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "def evaluate(encoder, decoder, input_tensor):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        encoder_hidden = encoder.initHidden(input_tensor.size(1))\n",
    "        input_length = input_tensor.size(0)\n",
    "\n",
    "        encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "        \n",
    "        # We run the input sequence through the encoder\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        \n",
    "#         print(encoder_outputs[:1])\n",
    "        decoder_input = torch.full((input_tensor.size(1),), BOS_IDX, device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(MAX_LENGTH, MAX_LENGTH)\n",
    "\n",
    "        for di in range(MAX_LENGTH):\n",
    "#             print(decoder_input)\n",
    "#             print(decoder_input.shape)\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            if topi.item() == EOS_IDX:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(english_id_to_text(topi))\n",
    "#             print(decoded_words)\n",
    "            decoder_input = topi.detach()\n",
    "          \n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s' % (asMinutes(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "def trainIters(encoder, decoder, epochs,initHIDDEN = True, print_every=50, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    PAD_IDX = en_vocab.stoi['<pad>']\n",
    "    criterion = nn.NLLLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(0, epochs +1):\n",
    "        print_loss_total = 0  # Reset every print_every\n",
    "        plot_loss_total = 0  # Reset every plot_every\n",
    "        print(\"-----------------------\")\n",
    "        print(\"EPOCH: \", epoch, \" out of \", epochs, \" epochs\")\n",
    "        print(\"-----------------------\")\n",
    "        for batch_counter, (src, trg, trg_output) in enumerate(train_iter):\n",
    "            src, trg, trg_output = src.to(device), trg.to(device), trg_output.to(device)\n",
    "            src_ = src.permute(1,0)\n",
    "            trg_ = trg.permute(1,0)\n",
    "            trg_output_ = trg_output.permute(1, 0)\n",
    "            \n",
    "            # Uncomment to go 1 by 1 manual unbatch version (preferably simply set batch = 1)\n",
    "#             for j in range(len(src_)):\n",
    "#                 loss = train(src_[j].unsqueeze(dim=1), trg_[j].unsqueeze(dim=1), trg_output_[j].unsqueeze(dim=1), encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "#                 print_loss_total += loss\n",
    "#                 plot_loss_total += loss\n",
    "                \n",
    "            loss = train(src, trg, trg_output, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, initHIDDEN)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "    #     training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "    #                       for i in range(n_iters)]\n",
    "\n",
    "            batch_counter += 1\n",
    "\n",
    "            if batch_counter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, batch_counter / len(train_iter)),\n",
    "                                             batch_counter, batch_counter/ len(train_iter) * 100, print_loss_avg))\n",
    "\n",
    "            if batch_counter % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 6,497,792 trainable parameters\n",
      "The model has 7,295,367 trainable parameters\n",
      "-----------------------\n",
      "EPOCH:  0  out of  5  epochs\n",
      "-----------------------\n",
      "4m 13s (50 22%) 4.7178\n",
      "8m 36s (100 44%) 3.4041\n",
      "13m 4s (150 66%) 3.1521\n",
      "17m 39s (200 88%) 3.1045\n",
      "-----------------------\n",
      "EPOCH:  1  out of  5  epochs\n",
      "-----------------------\n",
      "24m 45s (50 22%) 2.9957\n",
      "29m 28s (100 44%) 2.8606\n",
      "34m 14s (150 66%) 3.0066\n",
      "39m 4s (200 88%) 2.7396\n",
      "-----------------------\n",
      "EPOCH:  2  out of  5  epochs\n",
      "-----------------------\n",
      "46m 27s (50 22%) 2.7482\n",
      "51m 21s (100 44%) 2.7971\n",
      "56m 1s (150 66%) 2.7904\n",
      "60m 39s (200 88%) 2.7809\n",
      "-----------------------\n",
      "EPOCH:  3  out of  5  epochs\n",
      "-----------------------\n",
      "68m 4s (50 22%) 2.7880\n",
      "72m 54s (100 44%) 2.7448\n",
      "77m 45s (150 66%) 2.6871\n",
      "82m 36s (200 88%) 2.7618\n",
      "-----------------------\n",
      "EPOCH:  4  out of  5  epochs\n",
      "-----------------------\n",
      "90m 7s (50 22%) 2.5907\n",
      "94m 58s (100 44%) 2.7561\n",
      "99m 53s (150 66%) 2.6835\n",
      "104m 53s (200 88%) 2.6481\n",
      "-----------------------\n",
      "EPOCH:  5  out of  5  epochs\n",
      "-----------------------\n",
      "112m 36s (50 22%) 2.6235\n",
      "117m 27s (100 44%) 2.6262\n",
      "122m 19s (150 66%) 2.7749\n",
      "127m 8s (200 88%) 2.8106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "INPUT_DIM = len(de_vocab)\n",
    "OUTPUT_DIM = len(en_vocab)\n",
    "hidden_size = 256\n",
    "\n",
    "encoder1 = EncoderRNN(INPUT_DIM, hidden_size, 4).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, OUTPUT_DIM, 4, dropout_p=0.1).to(device)\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(encoder1):,} trainable parameters')\n",
    "print(f'The model has {count_parameters(attn_decoder1):,} trainable parameters')\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, epochs = 5, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "# Randomly evaluate\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for _, (src, trg, tgt_output) in enumerate(test_iter):\n",
    "        for i in range(n):\n",
    "#             print(src.shape)\n",
    "            src_ = src.permute(1,0)\n",
    "            trg_ = trg.permute(1,0)\n",
    "#             print(src_[i])\n",
    "#             print(src_[i].unsqueeze(dim=1).shape)\n",
    "\n",
    "            src_ = src_[i]\n",
    "            trg_ = trg_[i]\n",
    "            print('>', german_id_to_text(src_))\n",
    "            print('=', english_id_to_text(trg_))\n",
    "            output_words, attentions = evaluate(encoder, decoder, src_.unsqueeze(dim=1))\n",
    "#             output_sentence = ' '.join(output_words)\n",
    "            print('<', output_words)\n",
    "            print('')\n",
    "        break\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ['<bos>', 'Ein', 'kleiner', 'Junge', 'mit', 'Kochmütze', 'und', 'Schürze', 'schneidet', 'in', 'einer', 'Küche', 'Würstchen', '.', '\\n', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "= ['<bos>', 'A', 'young', 'boy', ',', 'wearing', 'a', 'chef', \"'s\", 'hat', 'and', 'apron', ',', 'is', 'cutting', 'sausages', 'in', 'a', 'kitchen', '.', '\\n', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "< [['A'], ['man'], ['in'], ['a'], ['a'], ['a'], ['a'], ['a'], ['.'], ['.'], ['.'], ['\\n'], ['\\n'], ['.'], '<EOS>']\n",
      "\n",
      "f\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1, 1)\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Translation is hard task!! ##\n",
    "\n",
    "The dataset we have + the parameters + training time is not really enough to train a machine translation network! \n",
    "\n",
    "Implement and test Pytorch’s own Transformer models for machine translation\n",
    "Analyze based on the output and measurements.\n",
    "Compare to provided example seq2seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got here\n"
     ]
    }
   ],
   "source": [
    "print('got here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, nhead = 2):\n",
    "        super(Encoder, self).__init__()\n",
    "        #self.ninp = ninp\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        encoder_layers = TransformerEncoderLayer(input_size, nhead, hidden_size)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers,2)\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)   \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size()[0]\n",
    "#         print(\"X :\", x.shape)\n",
    "        embedded = self.embedding(x)\n",
    "#         print(\"EMBEDD: \", embedded.shape)\n",
    "        embedded = embedded.view(1, batch_size, -1)\n",
    "#         print(\"EMBEDD VIEW: \", embedded.shape)\n",
    "        output = embedded\n",
    "        #output, hidden = self.gru(output, hidden)\n",
    "        hidden = self.transformer_encoder(embedded)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(2, batch_size, self.hidden_size, device=device)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, OUTPUT_DIM, nhead = 2):\n",
    "        super(Decoder, self).__init__()\n",
    "        #self.ninp = ninp\n",
    "        self.hidden_size = hidden_size\n",
    "        decoder_layers = nn.TransformerDecoderLayer(hidden_size, nhead)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layers,2)\n",
    "        self.embedding = nn.Embedding(hidden_size, OUTPUT_DIM)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size()[0]\n",
    "#         print(\"X :\", x.shape)\n",
    "        embedded = self.embedding(x)\n",
    "#         print(\"EMBEDD: \", embedded.shape)\n",
    "        embedded = embedded.view(1, batch_size, -1)\n",
    "#         print(\"EMBEDD VIEW: \", embedded.shape)\n",
    "        output = embedded\n",
    "        output = self.transformer_decoder(embedded)\n",
    "        output= F.log_softmax(self.out(output[0]), dim=1)\n",
    "        #output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(2, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,954,152,508 trainable parameters\n",
      "The model has 80,600 trainable parameters\n",
      "-----------------------\n",
      "EPOCH:  0  out of  5  epochs\n",
      "-----------------------\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-1db48b783e5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'The model has {count_parameters(attn_decoder1):,} trainable parameters'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mtrainIters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitHIDDEN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-caa19d94ccd1>\u001b[0m in \u001b[0;36mtrainIters\u001b[1;34m(encoder, decoder, epochs, initHIDDEN, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m#                 plot_loss_total += loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitHIDDEN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mprint_loss_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mplot_loss_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-a7fd90536143>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(input_tensor, target_tensor, target_tensor_out, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, initHIDDEN)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# We run the input sequence through the encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mei\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mencoder_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mei\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;31m#         print(encoder_output.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mei\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AMLdl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-4703357d3a6e>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, hidden)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m#output, hidden = self.gru(output, hidden)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AMLdl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AMLdl\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AMLdl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AMLdl\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0msee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mTransformer\u001b[0m \u001b[1;32mclass\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m         src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n\u001b[0m\u001b[0;32m    294\u001b[0m                               key_padding_mask=src_key_padding_mask)[0]\n\u001b[0;32m    295\u001b[0m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AMLdl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AMLdl\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[0;32m    978\u001b[0m                 v_proj_weight=self.v_proj_weight)\n\u001b[0;32m    979\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 980\u001b[1;33m             return F.multi_head_attention_forward(\n\u001b[0m\u001b[0;32m    981\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AMLdl\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[0;32m   4623\u001b[0m         )\n\u001b[0;32m   4624\u001b[0m     \u001b[0mtgt_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4625\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0membed_dim_to_check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4626\u001b[0m     \u001b[1;31m# allow MHA to have different sizes for the feature dimension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4627\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(de_vocab)\n",
    "OUTPUT_DIM = len(en_vocab)\n",
    "hidden_size = 4\n",
    "\n",
    "encoder1 = Encoder(INPUT_DIM, hidden_size).to(device)\n",
    "attn_decoder1 = Decoder(hidden_size, OUTPUT_DIM).to(device)\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(encoder1):,} trainable parameters')\n",
    "print(f'The model has {count_parameters(attn_decoder1):,} trainable parameters')\n",
    "\n",
    "#trainIters(encoder1, attn_decoder1,initHIDDEN = True, epochs = 5, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Skola\\lab\\MAU-AML-labs\\2-language-models-lab\\.data\\wikitext-2-v1.zip: 100%|████| 4.48M/4.48M [00:00<00:00, 4.64MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 383.99 | loss  8.17 | ppl  3522.19\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 375.24 | loss  6.92 | ppl  1011.43\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 375.82 | loss  6.46 | ppl   637.62\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 377.57 | loss  6.30 | ppl   546.79\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 371.38 | loss  6.19 | ppl   488.78\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 372.69 | loss  6.16 | ppl   473.61\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 372.81 | loss  6.12 | ppl   452.85\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 371.88 | loss  6.11 | ppl   449.84\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 372.42 | loss  6.02 | ppl   413.23\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 375.22 | loss  6.02 | ppl   410.68\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 376.52 | loss  5.91 | ppl   366.91\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 378.98 | loss  5.97 | ppl   392.58\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 377.32 | loss  5.95 | ppl   384.89\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 379.31 | loss  5.88 | ppl   358.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 1155.27s | valid loss  5.80 | valid ppl   330.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 404.94 | loss  5.87 | ppl   354.08\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 404.80 | loss  5.85 | ppl   348.55\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 404.54 | loss  5.66 | ppl   288.10\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 403.86 | loss  5.70 | ppl   298.42\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 402.27 | loss  5.65 | ppl   284.90\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 400.97 | loss  5.69 | ppl   294.52\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 405.29 | loss  5.69 | ppl   296.34\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 402.63 | loss  5.71 | ppl   302.36\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 390.14 | loss  5.65 | ppl   285.07\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 383.41 | loss  5.67 | ppl   289.24\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 378.51 | loss  5.55 | ppl   257.93\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 390.05 | loss  5.65 | ppl   284.06\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 385.26 | loss  5.65 | ppl   282.91\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 384.75 | loss  5.58 | ppl   264.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 1210.58s | valid loss  5.65 | valid ppl   285.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 394.30 | loss  5.61 | ppl   271.82\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 387.10 | loss  5.62 | ppl   274.70\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 386.41 | loss  5.42 | ppl   226.79\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 386.66 | loss  5.48 | ppl   240.33\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 385.39 | loss  5.43 | ppl   228.64\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 381.45 | loss  5.47 | ppl   237.11\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 382.23 | loss  5.48 | ppl   240.68\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 384.47 | loss  5.51 | ppl   246.87\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 408.42 | loss  5.46 | ppl   234.48\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 390.68 | loss  5.48 | ppl   239.02\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 389.27 | loss  5.36 | ppl   212.33\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 389.81 | loss  5.46 | ppl   234.85\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 383.51 | loss  5.47 | ppl   236.49\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 378.05 | loss  5.40 | ppl   221.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 1186.18s | valid loss  5.61 | valid ppl   274.27\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#code from https://pytorch.org/tutorials/beginner/transformer_tutorial.html \n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "import io\n",
    "import torch\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "for line in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter)\n",
    "\n",
    "def data_process(raw_text_iter):\n",
    "    data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                       dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)\n",
    "\n",
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "ntokens = len(vocab.stoi) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_last_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)\n",
    "\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AMLdl] *",
   "language": "python",
   "name": "conda-env-AMLdl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
