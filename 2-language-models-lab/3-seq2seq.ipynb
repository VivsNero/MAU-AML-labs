{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language translation with Torchtext!! ## \n",
    "\n",
    "Seq2Seq network with torchtext\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/torchtext_translation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys#\n",
    "stdout = sys.stdout#\n",
    "\n",
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import torchtext\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "import io\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import math\n",
    "import time\n",
    "\n",
    "sys.stdout = stdout #these lines are a workaround to fix an issue where no prints were showing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
    "\n",
    "de_tokenizer = get_tokenizer('spacy', language='de')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en')\n",
    "\n",
    "def build_vocab(filepath, tokenizer):\n",
    "    counter = Counter()\n",
    "    with io.open(filepath, encoding=\"utf8\") as f:\n",
    "        for string_ in f:\n",
    "            counter.update(tokenizer(string_))\n",
    "    \n",
    "    return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
    "\n",
    "def data_process(filepaths):\n",
    "    raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "    src_max_length = 0\n",
    "    tgt_max_length = 0\n",
    "    data = []\n",
    "    for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "        de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)], dtype=torch.long)\n",
    "        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long)\n",
    "        data.append((de_tensor_, en_tensor_))\n",
    "        \n",
    "        if de_tensor_.size(0) > src_max_length:\n",
    "            src_max_length = de_tensor_.size(0)\n",
    "            \n",
    "        if en_tensor_.size(0) > tgt_max_length:\n",
    "            tgt_max_length = en_tensor_.size(0)\n",
    "        \n",
    "    return data, src_max_length, tgt_max_length\n",
    "\n",
    "train_data, trsl, trtl = data_process(train_filepaths)\n",
    "val_data, vsl, vtl = data_process(val_filepaths)\n",
    "test_data, tesl, tetl = data_process(test_filepaths)\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "47\n",
      "f\n"
     ]
    }
   ],
   "source": [
    "# We calculate the sentence with max_length\n",
    "candidates_lengths = [trsl, trtl, vsl, vtl, tesl, tetl]\n",
    "print(max(candidates_lengths))\n",
    "MAX_LENGTH = max(candidates_lengths) + 2\n",
    "print(MAX_LENGTH)\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "BOS_IDX = de_vocab['<bos>']\n",
    "EOS_IDX = de_vocab['<eos>']\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    de_batch, en_batch, en_batch_out = [], [], []\n",
    "    a = 0\n",
    "    for (de_item, en_item) in data_batch:\n",
    "        de_extra = MAX_LENGTH - (de_item.size(0) + 2)\n",
    "        en_extra = MAX_LENGTH - (en_item.size(0) + 2)\n",
    "        en_inp_extra = MAX_LENGTH - (en_item.size(0) + 1)\n",
    "        \n",
    "        de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX]), torch.full((de_extra,), PAD_IDX)], dim=0))\n",
    "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX]), torch.full((en_extra,), PAD_IDX)], dim=0))\n",
    "        en_batch_out.append(torch.cat([en_item, torch.tensor([EOS_IDX]), torch.full((en_inp_extra,), PAD_IDX)], dim=0)) # Target input \n",
    "    de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    en_batch_out = pad_sequence(en_batch_out, padding_value=PAD_IDX)\n",
    "    return de_batch, en_batch, en_batch_out\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_id_to_text = lambda x: [de_vocab.itos[t] for t in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_id_to_text = lambda x: [en_vocab.itos[t] for t in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we create the Seq2Seq model ##\n",
    "\n",
    "The Seq2Seq model is an encoder-decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest seq2seq decoder we use only last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder’s last hidden state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, gru_layers):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.gru_layers = gru_layers\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, gru_layers)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size()[0]\n",
    "#         print(\"X :\", x.shape)\n",
    "        embedded = self.embedding(x)\n",
    "#         print(\"EMBEDD: \", embedded.shape)\n",
    "        embedded = embedded.view(1, batch_size, -1)\n",
    "#         print(\"EMBEDD VIEW: \", embedded.shape)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(self.gru_layers, batch_size, self.hidden_size, device=device)\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Decoder ###\n",
    "If only the context vector is passed betweeen the encoder and decoder, that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to “focus” on a different part of the encoder’s outputs for every step of the decoder’s own outputs. First we calculate a set of attention weights. These will be multiplied by the encoder output vectors to create a weighted combination. The result (called attn_applied in the code) should contain information about that specific part of the input sequence, and thus help the decoder choose the right output words.\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward layer attn, using the decoder’s input and hidden state as inputs. Because there are sentences of all sizes in the training data, to actually create and train this layer we have to choose a maximum sentence length (input length, for encoder outputs) that it can apply to. Sentences of the maximum length will use all the attention weights, while shorter sentences will only use the first few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, gru_layers, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.gru_layers = gru_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, gru_layers)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        batch_size = x.size()[0]\n",
    "        # Embedding: a simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "        # This module is often used to store word embeddings and retrieve them using indices. \n",
    "        # The input to the module is a list of indices, and the output is the corresponding word embeddings\n",
    "#         print(\"INPUT SHAPE: \", x.shape)\n",
    "#         print(batch_size)\n",
    "        embedded = self.embedding(x)\n",
    "#         print(\"EMBEDD: \", embedded.shape)\n",
    "        embedded = embedded.view(1, batch_size, -1)\n",
    "#         print(\"EMBEDD VIEW: \", embedded.shape)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        \n",
    "        # torch.bmm performs a batch matrix-matrix product of matrices stored in input and mat2.\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(self.gru_layers, batch_size, self.hidden_size, device=device)\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We  create the classic train/evaluate loop ###\n",
    "\n",
    "For this task, we have something called **teacher_forcing_ratio**, this helps us vary between giving the network the possibility to try to translate using its own previous prediction (no teacher forcing), or we use the known target for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, target_tensor_out, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, initHIDDEN = True):\n",
    "    \n",
    "    if initHIDDEN:\n",
    "        encoder_hidden = encoder.initHidden(input_tensor.size(1))\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    # We run the input sequence through the encoder\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "#         print(encoder_output.shape)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    \n",
    "    decoder_input = torch.full((target_tensor.size(1),), BOS_IDX, device=device)\n",
    "#     decoder_input = torch.tensor([[BOS_IDX], target_tensor.size(1)], device=device)\n",
    "\n",
    "#     print(decoder_input.shape)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoded_words = []\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoded_words.append(english_id_to_text(topi[0]))\n",
    "            loss += criterion(decoder_output, target_tensor_out[di])\n",
    "            if di + 1 < target_length:\n",
    "                decoder_input = target_tensor[di + 1]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoded_words.append(english_id_to_text(topi[0]))\n",
    "            decoder_input = topi.detach()  # detach from history as input\n",
    "#             print(target_tensor[di].shape)\n",
    "            loss += criterion(decoder_output, target_tensor_out[di])\n",
    "\n",
    "#     print(decoded_words)\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "def evaluate(encoder, decoder, input_tensor):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        encoder_hidden = encoder.initHidden(input_tensor.size(1))\n",
    "        input_length = input_tensor.size(0)\n",
    "\n",
    "        encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "        \n",
    "        # We run the input sequence through the encoder\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        \n",
    "#         print(encoder_outputs[:1])\n",
    "        decoder_input = torch.full((input_tensor.size(1),), BOS_IDX, device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(MAX_LENGTH, MAX_LENGTH)\n",
    "\n",
    "        for di in range(MAX_LENGTH):\n",
    "#             print(decoder_input)\n",
    "#             print(decoder_input.shape)\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            if topi.item() == EOS_IDX:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(english_id_to_text(topi))\n",
    "#             print(decoded_words)\n",
    "            decoder_input = topi.detach()\n",
    "          \n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s' % (asMinutes(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "def trainIters(encoder, decoder, epochs,initHIDDEN = True, print_every=50, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    PAD_IDX = en_vocab.stoi['<pad>']\n",
    "    criterion = nn.NLLLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(0, epochs +1):\n",
    "        print_loss_total = 0  # Reset every print_every\n",
    "        plot_loss_total = 0  # Reset every plot_every\n",
    "        print(\"-----------------------\")\n",
    "        print(\"EPOCH: \", epoch, \" out of \", epochs, \" epochs\")\n",
    "        print(\"-----------------------\")\n",
    "        for batch_counter, (src, trg, trg_output) in enumerate(train_iter):\n",
    "            src, trg, trg_output = src.to(device), trg.to(device), trg_output.to(device)\n",
    "            src_ = src.permute(1,0)\n",
    "            trg_ = trg.permute(1,0)\n",
    "            trg_output_ = trg_output.permute(1, 0)\n",
    "            \n",
    "            # Uncomment to go 1 by 1 manual unbatch version (preferably simply set batch = 1)\n",
    "#             for j in range(len(src_)):\n",
    "#                 loss = train(src_[j].unsqueeze(dim=1), trg_[j].unsqueeze(dim=1), trg_output_[j].unsqueeze(dim=1), encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "#                 print_loss_total += loss\n",
    "#                 plot_loss_total += loss\n",
    "                \n",
    "            loss = train(src, trg, trg_output, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, initHIDDEN)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "    #     training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "    #                       for i in range(n_iters)]\n",
    "\n",
    "            batch_counter += 1\n",
    "\n",
    "            if batch_counter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, batch_counter / len(train_iter)),\n",
    "                                             batch_counter, batch_counter/ len(train_iter) * 100, print_loss_avg))\n",
    "\n",
    "            if batch_counter % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 6,497,792 trainable parameters\n",
      "The model has 7,295,367 trainable parameters\n",
      "-----------------------\n",
      "EPOCH:  0  out of  5  epochs\n",
      "-----------------------\n",
      "5m 27s (50 22%) 4.4037\n",
      "11m 16s (100 44%) 3.3256\n",
      "17m 7s (150 66%) 3.2630\n",
      "23m 2s (200 88%) 2.9897\n",
      "-----------------------\n",
      "EPOCH:  1  out of  5  epochs\n",
      "-----------------------\n",
      "32m 7s (50 22%) 3.0885\n",
      "38m 1s (100 44%) 2.9875\n",
      "44m 0s (150 66%) 2.9231\n",
      "50m 2s (200 88%) 2.8418\n",
      "-----------------------\n",
      "EPOCH:  2  out of  5  epochs\n",
      "-----------------------\n",
      "58m 55s (50 22%) 2.8222\n",
      "65m 4s (100 44%) 2.7374\n",
      "71m 23s (150 66%) 2.7423\n",
      "77m 37s (200 88%) 2.8127\n",
      "-----------------------\n",
      "EPOCH:  3  out of  5  epochs\n",
      "-----------------------\n",
      "87m 14s (50 22%) 2.7855\n",
      "93m 47s (100 44%) 2.6796\n",
      "100m 18s (150 66%) 2.7241\n",
      "106m 53s (200 88%) 2.7150\n",
      "-----------------------\n",
      "EPOCH:  4  out of  5  epochs\n",
      "-----------------------\n",
      "115m 30s (50 22%) 2.6404\n",
      "121m 17s (100 44%) 2.7987\n",
      "126m 39s (150 66%) 2.7943\n",
      "131m 17s (200 88%) 2.6606\n",
      "-----------------------\n",
      "EPOCH:  5  out of  5  epochs\n",
      "-----------------------\n",
      "137m 41s (50 22%) 2.7416\n",
      "141m 50s (100 44%) 2.6356\n",
      "145m 59s (150 66%) 2.5912\n",
      "150m 7s (200 88%) 2.6816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "INPUT_DIM = len(de_vocab)\n",
    "OUTPUT_DIM = len(en_vocab)\n",
    "hidden_size = 256\n",
    "\n",
    "encoder1 = EncoderRNN(INPUT_DIM, hidden_size, 4).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, OUTPUT_DIM, 4, dropout_p=0.1).to(device)\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(encoder1):,} trainable parameters')\n",
    "print(f'The model has {count_parameters(attn_decoder1):,} trainable parameters')\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, epochs = 5, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "# Randomly evaluate\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for _, (src, trg, tgt_output) in enumerate(test_iter):\n",
    "        for i in range(n):\n",
    "#             print(src.shape)\n",
    "            src_ = src.permute(1,0)\n",
    "            trg_ = trg.permute(1,0)\n",
    "#             print(src_[i])\n",
    "#             print(src_[i].unsqueeze(dim=1).shape)\n",
    "\n",
    "            src_ = src_[i]\n",
    "            trg_ = trg_[i]\n",
    "            print('>', german_id_to_text(src_))\n",
    "            print('=', english_id_to_text(trg_))\n",
    "            output_words, attentions = evaluate(encoder, decoder, src_.unsqueeze(dim=1))\n",
    "#             output_sentence = ' '.join(output_words)\n",
    "            print('<', output_words)\n",
    "            print('')\n",
    "        break\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1, 1)\n",
    "print('f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2.4\n",
    "## Machine Translation is hard task!! ##\n",
    "\n",
    "The dataset we have + the parameters + training time is not really enough to train a machine translation network! \n",
    "\n",
    "Implement and test Pytorch’s own Transformer models for machine translation\n",
    "Analyze based on the output and measurements.\n",
    "Compare to provided example seq2seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got here\n"
     ]
    }
   ],
   "source": [
    "print('got here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, nhead = 2):\n",
    "        super(Encoder, self).__init__()\n",
    "        #self.ninp = ninp\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        encoder_layers = TransformerEncoderLayer(input_size, nhead, hidden_size)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers,2)\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)   \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size()[0]\n",
    "#         print(\"X :\", x.shape)\n",
    "        embedded = self.embedding(x)\n",
    "#         print(\"EMBEDD: \", embedded.shape)\n",
    "        embedded = embedded.view(1, batch_size, -1)\n",
    "#         print(\"EMBEDD VIEW: \", embedded.shape)\n",
    "        output = embedded\n",
    "        #output, hidden = self.gru(output, hidden)\n",
    "        hidden = self.transformer_encoder(embedded)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(2, batch_size, self.hidden_size, device=device)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, OUTPUT_DIM, nhead = 2):\n",
    "        super(Decoder, self).__init__()\n",
    "        #self.ninp = ninp\n",
    "        self.hidden_size = hidden_size\n",
    "        decoder_layers = nn.TransformerDecoderLayer(hidden_size, nhead)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layers,2)\n",
    "        self.embedding = nn.Embedding(hidden_size, OUTPUT_DIM)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size()[0]\n",
    "#         print(\"X :\", x.shape)\n",
    "        embedded = self.embedding(x)\n",
    "#         print(\"EMBEDD: \", embedded.shape)\n",
    "        embedded = embedded.view(1, batch_size, -1)\n",
    "#         print(\"EMBEDD VIEW: \", embedded.shape)\n",
    "        output = embedded\n",
    "        output = self.transformer_decoder(embedded)\n",
    "        output= F.log_softmax(self.out(output[0]), dim=1)\n",
    "        #output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(2, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,954,152,508 trainable parameters\n",
      "The model has 80,600 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(de_vocab)\n",
    "OUTPUT_DIM = len(en_vocab)\n",
    "hidden_size = 4\n",
    "\n",
    "encoder1 = Encoder(INPUT_DIM, hidden_size).to(device)\n",
    "attn_decoder1 = Decoder(hidden_size, OUTPUT_DIM).to(device)\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(encoder1):,} trainable parameters')\n",
    "print(f'The model has {count_parameters(attn_decoder1):,} trainable parameters')\n",
    "\n",
    "#trainIters(encoder1, attn_decoder1,initHIDDEN = True, epochs = 5, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 375.05 | loss  8.11 | ppl  3336.87\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 360.09 | loss  6.89 | ppl   979.65\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 366.38 | loss  6.46 | ppl   637.15\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 361.52 | loss  6.31 | ppl   552.47\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 361.27 | loss  6.19 | ppl   488.35\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 361.63 | loss  6.16 | ppl   474.86\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 364.15 | loss  6.12 | ppl   453.76\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 363.95 | loss  6.11 | ppl   450.58\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 362.32 | loss  6.03 | ppl   414.87\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 366.99 | loss  6.02 | ppl   411.30\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 415.24 | loss  5.90 | ppl   365.64\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 403.62 | loss  5.98 | ppl   394.25\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 383.62 | loss  5.96 | ppl   387.77\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 382.93 | loss  5.89 | ppl   359.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 1148.03s | valid loss  5.79 | valid ppl   327.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 383.85 | loss  5.87 | ppl   354.15\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 382.83 | loss  5.85 | ppl   348.79\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 379.84 | loss  5.67 | ppl   290.43\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 375.90 | loss  5.71 | ppl   302.20\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 376.35 | loss  5.66 | ppl   285.92\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 375.85 | loss  5.69 | ppl   295.56\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 367.02 | loss  5.69 | ppl   297.11\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 362.18 | loss  5.71 | ppl   303.03\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 375.16 | loss  5.65 | ppl   285.67\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 380.90 | loss  5.67 | ppl   291.14\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 385.79 | loss  5.56 | ppl   261.12\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 387.00 | loss  5.65 | ppl   284.85\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 389.42 | loss  5.65 | ppl   283.90\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 389.34 | loss  5.59 | ppl   266.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 1165.51s | valid loss  5.66 | valid ppl   287.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 387.78 | loss  5.61 | ppl   271.85\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 404.49 | loss  5.63 | ppl   277.61\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 401.29 | loss  5.43 | ppl   228.68\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 409.42 | loss  5.49 | ppl   242.13\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 413.68 | loss  5.44 | ppl   230.27\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 414.10 | loss  5.47 | ppl   238.00\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 439.92 | loss  5.49 | ppl   242.59\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 451.14 | loss  5.52 | ppl   248.84\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 442.76 | loss  5.47 | ppl   238.18\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 443.02 | loss  5.49 | ppl   241.06\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 439.02 | loss  5.37 | ppl   214.14\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 437.50 | loss  5.47 | ppl   236.81\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 441.31 | loss  5.47 | ppl   238.62\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 438.99 | loss  5.41 | ppl   223.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 1309.64s | valid loss  5.59 | valid ppl   268.36\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#code from https://pytorch.org/tutorials/beginner/transformer_tutorial.html \n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "import io\n",
    "import torch\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "for line in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter)\n",
    "\n",
    "def data_process(raw_text_iter):\n",
    "    data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                       dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)\n",
    "\n",
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "ntokens = len(vocab.stoi) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_last_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)\n",
    "\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AMLdl] *",
   "language": "python",
   "name": "conda-env-AMLdl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
